{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pygallica-autobib Automatically match Bibliographies against bnf.gallica.fr! Overview gallica_autobib will match your bibliographies against the French National Library and download articles as pdfs if possible, optionally post-processing them. Whilst it obviously cannot download articles which Gallica does not hold, it strives to achieve a 100% match rate. If you find an article it does not match, please report a bug . Features: Input in RIS or Bibtex format Output report generated with a jinja template (templates for org-mode, html and plaintext supplied) gallica_autobib is designed to automate otherwise tedious work. In this respect it is subject to the well-known laws of automation . Online Demo There is an online demo with a very basic interface. It is not currently production ready. Note that Gallica use some very aggressive rate limiting, so if you hit it with the same requests too often it will simply go down. Installation You need python >= 3.9 (but if anyone needs to use this with an older python, open an issue and I will refactor the few incompatible statements). Then install as usual: pipx install gallica-autobib # prefered, if you have pipx, or python -m pip install gallica-autobib Standalone Usage gallica-autobib my-bibliography.bib pdfs # match my-bibliography and put files in ./pdfs gallica-autobib --help As a library from pathlib import Path from gallica_autobib.models import Article from gallica_autobib.query import Query , GallicaResource target = Article ( journaltitle = \"La Vie spirituelle\" , author = \"M.-D. Chenu\" , pages = list ( range ( 547 , 552 )), volume = 7 , year = 1923 , title = \"Asc\u00e8se et p\u00e9ch\u00e9 originel\" , ) query = Query ( target ) candidate = Query . run () . candidate # get candidate journal gallica_resource = GallicaResource ( candidate , source ) ark = gallica_resource . ark # match candidate article gallica_resource . download_pdf ( Path ( \"article.pdf\" )) or if you just want to do what the cli does: from pathlib import Path from gallica_resource.pipeline import BibtexParser parser = BibtexParser ( Path ( \"outdir\" )) with Path ( \"articles.bib\" ) . open () as f : parser . read ( f ) parser . run () for result in parser . results : print ( result ) for more advanced usage see the documentation and the test suite. Developing # ensure you have Poetry installed pip install --user poetry # install all dependencies (including dev) poetry install When your feature is ready, open a PR. Testing We use pytest and mypy. You may want to focus on getting unit tests passing first: poetry run pytest --cov = gallica_autobib --cov-report html --cov-branch tests If you have started a shell with poetry shell you can drop the poetry run . When unittests are passing you can run the whole suite with: poetry run scripts/test.sh Note that tests will only pass fully if /tmp/ exists and is writeable, and if poppler and imagemagick are installed. This is due to the use of pdf-diff-visually and the rather hackish template tests. You may wish to check your formatting first with poetry run scripts/format.sh Alternatively, just open a premature PR to run the tests in CI. Note that this is rather slow. Plausibly Askable Questions Why don't you also check xyz.com? Because I don't know about it. Open an issue and I'll look into it. Why don't you use library xyz for image processing? Probably because I don't know about it. This is a quick tool written to help me research. Submit a PR and I'll happily update it. Why is the code so verbose? It is rather object-oriented. It might be rather over engineered. It was written in a hurry and the design evolved as it went along. On the other hand, it should be easier to extend. Why not just extend pygallica? I mean to submit the SRU stuff as an extension to pygallica","title":"About"},{"location":"#pygallica-autobib","text":"Automatically match Bibliographies against bnf.gallica.fr!","title":"pygallica-autobib"},{"location":"#overview","text":"gallica_autobib will match your bibliographies against the French National Library and download articles as pdfs if possible, optionally post-processing them. Whilst it obviously cannot download articles which Gallica does not hold, it strives to achieve a 100% match rate. If you find an article it does not match, please report a bug . Features: Input in RIS or Bibtex format Output report generated with a jinja template (templates for org-mode, html and plaintext supplied) gallica_autobib is designed to automate otherwise tedious work. In this respect it is subject to the well-known laws of automation .","title":"Overview"},{"location":"#online-demo","text":"There is an online demo with a very basic interface. It is not currently production ready. Note that Gallica use some very aggressive rate limiting, so if you hit it with the same requests too often it will simply go down.","title":"Online Demo"},{"location":"#installation","text":"You need python >= 3.9 (but if anyone needs to use this with an older python, open an issue and I will refactor the few incompatible statements). Then install as usual: pipx install gallica-autobib # prefered, if you have pipx, or python -m pip install gallica-autobib","title":"Installation"},{"location":"#standalone-usage","text":"gallica-autobib my-bibliography.bib pdfs # match my-bibliography and put files in ./pdfs gallica-autobib --help","title":"Standalone Usage"},{"location":"#as-a-library","text":"from pathlib import Path from gallica_autobib.models import Article from gallica_autobib.query import Query , GallicaResource target = Article ( journaltitle = \"La Vie spirituelle\" , author = \"M.-D. Chenu\" , pages = list ( range ( 547 , 552 )), volume = 7 , year = 1923 , title = \"Asc\u00e8se et p\u00e9ch\u00e9 originel\" , ) query = Query ( target ) candidate = Query . run () . candidate # get candidate journal gallica_resource = GallicaResource ( candidate , source ) ark = gallica_resource . ark # match candidate article gallica_resource . download_pdf ( Path ( \"article.pdf\" )) or if you just want to do what the cli does: from pathlib import Path from gallica_resource.pipeline import BibtexParser parser = BibtexParser ( Path ( \"outdir\" )) with Path ( \"articles.bib\" ) . open () as f : parser . read ( f ) parser . run () for result in parser . results : print ( result ) for more advanced usage see the documentation and the test suite.","title":"As a library"},{"location":"#developing","text":"# ensure you have Poetry installed pip install --user poetry # install all dependencies (including dev) poetry install When your feature is ready, open a PR.","title":"Developing"},{"location":"#testing","text":"We use pytest and mypy. You may want to focus on getting unit tests passing first: poetry run pytest --cov = gallica_autobib --cov-report html --cov-branch tests If you have started a shell with poetry shell you can drop the poetry run . When unittests are passing you can run the whole suite with: poetry run scripts/test.sh Note that tests will only pass fully if /tmp/ exists and is writeable, and if poppler and imagemagick are installed. This is due to the use of pdf-diff-visually and the rather hackish template tests. You may wish to check your formatting first with poetry run scripts/format.sh Alternatively, just open a premature PR to run the tests in CI. Note that this is rather slow.","title":"Testing"},{"location":"#plausibly-askable-questions","text":"","title":"Plausibly Askable Questions"},{"location":"#why-dont-you-also-check-xyzcom","text":"Because I don't know about it. Open an issue and I'll look into it.","title":"Why don't you also check xyz.com?"},{"location":"#why-dont-you-use-library-xyz-for-image-processing","text":"Probably because I don't know about it. This is a quick tool written to help me research. Submit a PR and I'll happily update it.","title":"Why don't you use library xyz for image processing?"},{"location":"#why-is-the-code-so-verbose","text":"It is rather object-oriented. It might be rather over engineered. It was written in a hurry and the design evolved as it went along. On the other hand, it should be easier to extend.","title":"Why is the code so verbose?"},{"location":"#why-not-just-extend-pygallica","text":"I mean to submit the SRU stuff as an extension to pygallica","title":"Why not just extend pygallica?"},{"location":"caching/","text":"Caching Ratelimiting Gallica appears to defend its resources quite aggressively. too many requests for any endpoint will result in a resource temporarily unavailable error. too many requests for a document will result either in a timeout, or in the document being replaced with a png (sic!) containing the text (in French) 'resource unavailable; please try again later'. Presumably this is designed to catch out scripts trying to mirror the service, since a browser will just display the image. service speed appears to be batched, i.e. the first n. queries are quite quick, the next n. slower, and so on. We try to work around these issues with a proper downloader with exponential backoff (replacing gallipy 's use of requests), a long (5 min!) pause if we detect the png ratelimiting, and by caching as much as possible. Gallica is a wonderful service and it would be a pity to ddos it, although one would think a catalogue api should be able to handle large numbers of parallel queries. File caching If --no-clean is specified to the cli (or clean is set to False ) the original pdf will be left around. If a pdf of the desire name exists the download will be skipped. If skip_existing=True is passed on the cli or set in process_args , an existing processed pdf will cause processing to be skipped. These measures speed up repeated queries massively . Match caching Match caching is implemented with an sqlite database ~/.cache/gallica_autobib/ which uses the repr() of the target as a key (alright, it uses a SHA1 hash of the target's repr() ). So any changes in the target (i.e. any changes in the bibliography you supply for this entry) will result in a rematch. This is the simplest way to ensure reliability. Enough data is cached to prevent the GallicaResource() object doing any requests. The entire Match() object is cached, as output templates can use it directly. Sometimes you may wish to update the cache: in this case running with --ignore-cache will cause it to be overwritten.","title":"Caching"},{"location":"caching/#caching","text":"","title":"Caching"},{"location":"caching/#ratelimiting","text":"Gallica appears to defend its resources quite aggressively. too many requests for any endpoint will result in a resource temporarily unavailable error. too many requests for a document will result either in a timeout, or in the document being replaced with a png (sic!) containing the text (in French) 'resource unavailable; please try again later'. Presumably this is designed to catch out scripts trying to mirror the service, since a browser will just display the image. service speed appears to be batched, i.e. the first n. queries are quite quick, the next n. slower, and so on. We try to work around these issues with a proper downloader with exponential backoff (replacing gallipy 's use of requests), a long (5 min!) pause if we detect the png ratelimiting, and by caching as much as possible. Gallica is a wonderful service and it would be a pity to ddos it, although one would think a catalogue api should be able to handle large numbers of parallel queries.","title":"Ratelimiting"},{"location":"caching/#file-caching","text":"If --no-clean is specified to the cli (or clean is set to False ) the original pdf will be left around. If a pdf of the desire name exists the download will be skipped. If skip_existing=True is passed on the cli or set in process_args , an existing processed pdf will cause processing to be skipped. These measures speed up repeated queries massively .","title":"File caching"},{"location":"caching/#match-caching","text":"Match caching is implemented with an sqlite database ~/.cache/gallica_autobib/ which uses the repr() of the target as a key (alright, it uses a SHA1 hash of the target's repr() ). So any changes in the target (i.e. any changes in the bibliography you supply for this entry) will result in a rematch. This is the simplest way to ensure reliability. Enough data is cached to prevent the GallicaResource() object doing any requests. The entire Match() object is cached, as output templates can use it directly. Sometimes you may wish to update the cache: in this case running with --ignore-cache will cause it to be overwritten.","title":"Match caching"},{"location":"image_processing/","text":"Background Automagic image processing is very hard. There are some good libraries out there to play with. Gallica-Autobib very deliberately does not strive for a general solution to the scanned image problem. This problem can be broken down into several parts: Identifying the region of the scan which contains the page. Identifying the region of the page which contains the content. Dewarping, deskewing, deblurring etc. the content region. Transforming the content region to coerce it into the desired output format (posterizing, thresholding, etc). The steps do not have to be done in this order, although this is the order in which we do them. Additionally there is an implied crop between 3 and 4, but cropping is the least of our problems. If you have a bunch of generic scans to process, I recommend you stop reading and use something like scantailor to do it in batch mode. You might like to use a set of scripts I developed to automate as much as possible. Cropping to content Gallica makes our life slightly easier: Almost all scans are OCRd. OCR data contains a \"content box\". We can get this straight from the server, in xml (ugh). Scans are in general of very high quality. Initially I thought we could just take the content box and have done with it. Unfortunately that doesn't work: the OCR seems to be set up not to pick up page number. So we can reduce the problem to the following: Look for a single line of text outside the content box. This implies the ability to distinguish between lines of text and the edge of the page or any other noise the scan might have. After playing a bit with matplotlib the easiest way was just to take the mean of each row, differentiate it, and then detect the rapid changes in gradient which happen with a line of text. The line detection algorithm We start with Gallica's content box, extracted from the OCR endpoint: We grayscale it, take the mean of every row and then take the first differential of that mean: That differential is very useful since it gives us a distinctive signature for a line: a large spike in one direction followed by a large spike in the other, i.e. white-to-black followed by black-to-white. We look 1.5 standard deviations away from the mean of the differential: And now we can easily find a line of text. But we might still mistakenly detect the edge of the page. So we define a search region: This region is arbitrarily sized as 5% the height of the page either side of the text box. We then go through the region either side, backwards from the textbox. Going up we look for a positive and then a negative peak, going down we look for a negative then a positive peak. Thus we end up with: And then we grow the region manually to: Which is pretty decent. The script which produced this can be found in the repository as test_image_processing.py . The crude algorithm We have another algorithm which tries to crop to the content if given the page . It can fail if given the scan , as the edges of the page confuse it. This algorithm: grayscales the image autocontrasts it thresholds it aggressively returns the bounding box of black on the page This crop data can then be applied to the original image. Since we have two means of getting crop data, and the OCR method is pretty reliable (assuming Gallica's OCR coverage is extensive enough) we compare the two and raise an exception if the difference is too great. Depending on the mode this will either trigger manual intervention to select between the two regions, cause both to be added to the PDF for later processing, or stop execution. Processing Processing is distinct from cropping. Since there is no easy way to replace in an image in a pdf whilst leaving the text intact, processing the image (beyond cropping) implies losing Gallica's ocrd text. If you want both a nicely processed pdf for printing and a cropped, OCRd pdf for screen usage, run Gallica-Autobib twice, the first time with --no-clean . Caching will prevent it doing anything except reprocessing (unless there were unsuccessful matches). Processing consists in: cropping to content autocontrasting posterizing to reduce the problem domain filtering with a crude skewing filter autocontrasting again The 'crude skewing filter' is a slightly nicer version of thresholding. It multiplies values above a threshold by a multiplier, and divides those below a threshold by a divisor. Since the resulting data is rounded, this lets us squash the upper and lower ranges by different amounts, whilst still sending the image massively in the direction of a binary image. There is a good deal of scope for improvement with this algorithm: it was chosen because it is quick and easy. Title Pages Gallica gives us one of those annoying title pages; we crop it to the maximum dimensions of the final pdf and stick it on the front, since we should probably keep it. It's easy enough to remove for e.g. booklet generation, (indeed there's an option to do so) but I'm not paying your legal fees if anyone comes after you.","title":"Processing Downloads"},{"location":"image_processing/#background","text":"Automagic image processing is very hard. There are some good libraries out there to play with. Gallica-Autobib very deliberately does not strive for a general solution to the scanned image problem. This problem can be broken down into several parts: Identifying the region of the scan which contains the page. Identifying the region of the page which contains the content. Dewarping, deskewing, deblurring etc. the content region. Transforming the content region to coerce it into the desired output format (posterizing, thresholding, etc). The steps do not have to be done in this order, although this is the order in which we do them. Additionally there is an implied crop between 3 and 4, but cropping is the least of our problems. If you have a bunch of generic scans to process, I recommend you stop reading and use something like scantailor to do it in batch mode. You might like to use a set of scripts I developed to automate as much as possible.","title":"Background"},{"location":"image_processing/#cropping-to-content","text":"Gallica makes our life slightly easier: Almost all scans are OCRd. OCR data contains a \"content box\". We can get this straight from the server, in xml (ugh). Scans are in general of very high quality. Initially I thought we could just take the content box and have done with it. Unfortunately that doesn't work: the OCR seems to be set up not to pick up page number. So we can reduce the problem to the following: Look for a single line of text outside the content box. This implies the ability to distinguish between lines of text and the edge of the page or any other noise the scan might have. After playing a bit with matplotlib the easiest way was just to take the mean of each row, differentiate it, and then detect the rapid changes in gradient which happen with a line of text.","title":"Cropping to content"},{"location":"image_processing/#the-line-detection-algorithm","text":"We start with Gallica's content box, extracted from the OCR endpoint: We grayscale it, take the mean of every row and then take the first differential of that mean: That differential is very useful since it gives us a distinctive signature for a line: a large spike in one direction followed by a large spike in the other, i.e. white-to-black followed by black-to-white. We look 1.5 standard deviations away from the mean of the differential: And now we can easily find a line of text. But we might still mistakenly detect the edge of the page. So we define a search region: This region is arbitrarily sized as 5% the height of the page either side of the text box. We then go through the region either side, backwards from the textbox. Going up we look for a positive and then a negative peak, going down we look for a negative then a positive peak. Thus we end up with: And then we grow the region manually to: Which is pretty decent. The script which produced this can be found in the repository as test_image_processing.py .","title":"The line detection algorithm"},{"location":"image_processing/#the-crude-algorithm","text":"We have another algorithm which tries to crop to the content if given the page . It can fail if given the scan , as the edges of the page confuse it. This algorithm: grayscales the image autocontrasts it thresholds it aggressively returns the bounding box of black on the page This crop data can then be applied to the original image. Since we have two means of getting crop data, and the OCR method is pretty reliable (assuming Gallica's OCR coverage is extensive enough) we compare the two and raise an exception if the difference is too great. Depending on the mode this will either trigger manual intervention to select between the two regions, cause both to be added to the PDF for later processing, or stop execution.","title":"The crude algorithm"},{"location":"image_processing/#processing","text":"Processing is distinct from cropping. Since there is no easy way to replace in an image in a pdf whilst leaving the text intact, processing the image (beyond cropping) implies losing Gallica's ocrd text. If you want both a nicely processed pdf for printing and a cropped, OCRd pdf for screen usage, run Gallica-Autobib twice, the first time with --no-clean . Caching will prevent it doing anything except reprocessing (unless there were unsuccessful matches). Processing consists in: cropping to content autocontrasting posterizing to reduce the problem domain filtering with a crude skewing filter autocontrasting again The 'crude skewing filter' is a slightly nicer version of thresholding. It multiplies values above a threshold by a multiplier, and divides those below a threshold by a divisor. Since the resulting data is rounded, this lets us squash the upper and lower ranges by different amounts, whilst still sending the image massively in the direction of a binary image. There is a good deal of scope for improvement with this algorithm: it was chosen because it is quick and easy.","title":"Processing"},{"location":"image_processing/#title-pages","text":"Gallica gives us one of those annoying title pages; we crop it to the maximum dimensions of the final pdf and stick it on the front, since we should probably keep it. It's easy enough to remove for e.g. booklet generation, (indeed there's an option to do so) but I'm not paying your legal fees if anyone comes after you.","title":"Title Pages"},{"location":"matching/","text":"Matching Gallica's APIs Gallica has excellent API support. In fact, it has far too much API support: there seem to be several ways to get most information from it. The apis are documented extensively online . What is more impressive is the list of available bindings . Of these bindings we use gallipy simply because it seemed the easiest to get going. Gallipy has no search capabilities, so we handle the search directly at the SRU endpoint with sruthi . (Incidentally, the reason Gallica has such good API access is that one thing the French Government gets right is API access. See https://api.gouv.fr/rechercher-api for an idea of quite how much data is available.) Search Algorithm Gallica Autobib was originally written to handle the specific case of fetching journal articles. As far as I can tell the BNF only indexes physical holdings, i.e. you can ask it what journals it has, but not what articles they contain. Sometimes those holdings have TOCs, but the TOCs are extracted from the holding itself (possibly with manual intervention) and are thus not necessarily massively useful, as well as not being ubiquitous. Article Search Algorithm Currently this is the only algorithm implemented, although the other record types will be added later. We break the problem into several steps: find the right journal. Gallica has very reliable metadata, and we can trust it not to list the same journal under two different ids. find the right issue of this journal, i.e. the issue containing the article we want. Frequently step 1. succeeds and step 2. fails because Gallica doesn't hold the issue we want (particularly with the war years). Theoretically we should detect this condition properly and report on it, but for now we just fail. Finding the right issue is trickier, because we can't trust the year properly. Years can be off in general and journals are frequently collected into volumes, which for some reason seem to be offset by half a year. So for a journal in 1936 there are three years to search: 1935, in case the original number was published in 1936 but part of a volume starting in 1935, 1936, and 1937 (in case the converse of the first case holds). So we get the TOC of all three journals, and parse it. Unfortunately now we discover that Gallica's API excellence is only skin-deep: the TOC is only available in an XML more akin to HTML than a machine-friendly index to a document. So it has to be scraped, and then we look to see whether or not there is an article with the right title beginning on the right page: if so, the issue is a match. If this fails (perhaps because there is no TOC) we try fetching the page the article ought to be on, and doing a fuzzy search for the title. (Fortunately Gallica provides a pagination api to convert page numbers into views. Unfortunately it returns unstructured html, with the conversion as a bunch of cells in a list, so we iterate over them. We don't do this often enough for it to be worth building a mapping.) If this matches we try a fuzzy search for the author on the first page of the article, and the last if that fails. These matches are scored as the levenshtein distance (ratio) of the longest matching fuzzy string against the target, so matching 'de la' will not cause a false positive. On the other hand for authors with short names you really do need to supply them the way they sign themselves, since there's no automagic way to work out an abbreviation (at any rate, no easy way to do it which wouldn't trigger false positives without a much cleverer heuristic than Levenshtein distance). For long/obscure names this shouldn't be a problem. As an example, Marie-Dominique Chenu failed for M.-D. Chenu , but R\u00e9ginald Garrigou-Lagrange matched R. Garrigou-Lagrange O. P. without any difficulty. (Can you tell I'm a theologian?) This text matching uses the ocr text endpoint. Unfortunately, this returns the unformatted plaintext wrapped in an html document . The server crashes 1 if you requests text/plain or application/xml , but if you ask for application/json it serves up the same html page in a json object (without decomposing it at all). So out comes Beautiful Soup again, and we can get the text back out of the html. This is not quite using a regex to parse html , though actually the source is simple enough we could probably do that. Let's not, though. Matching other resources Is rather easier, since we basically just use the journal matching algorithm above. All the pieces are in place to do this, but it has not yet been tested, as I have not yet wanted to fetch a book or journal which Gallica holds. Match() objects We could have just put this algorithm in a function somewhere. But tuning heuristics when everything's in a function gets difficult, so we use Match() objects to keep track of the various different criteria. Currently everything gets equal weighting, but this will doubtless require tuning as time goes on. We also set a minimum threshold below which to fail. Note, crashes, and gives a very unhelpful error message. \u21a9","title":"Matching"},{"location":"matching/#matching","text":"","title":"Matching"},{"location":"matching/#gallicas-apis","text":"Gallica has excellent API support. In fact, it has far too much API support: there seem to be several ways to get most information from it. The apis are documented extensively online . What is more impressive is the list of available bindings . Of these bindings we use gallipy simply because it seemed the easiest to get going. Gallipy has no search capabilities, so we handle the search directly at the SRU endpoint with sruthi . (Incidentally, the reason Gallica has such good API access is that one thing the French Government gets right is API access. See https://api.gouv.fr/rechercher-api for an idea of quite how much data is available.)","title":"Gallica's APIs"},{"location":"matching/#search-algorithm","text":"Gallica Autobib was originally written to handle the specific case of fetching journal articles. As far as I can tell the BNF only indexes physical holdings, i.e. you can ask it what journals it has, but not what articles they contain. Sometimes those holdings have TOCs, but the TOCs are extracted from the holding itself (possibly with manual intervention) and are thus not necessarily massively useful, as well as not being ubiquitous.","title":"Search Algorithm"},{"location":"matching/#article-search-algorithm","text":"Currently this is the only algorithm implemented, although the other record types will be added later. We break the problem into several steps: find the right journal. Gallica has very reliable metadata, and we can trust it not to list the same journal under two different ids. find the right issue of this journal, i.e. the issue containing the article we want. Frequently step 1. succeeds and step 2. fails because Gallica doesn't hold the issue we want (particularly with the war years). Theoretically we should detect this condition properly and report on it, but for now we just fail. Finding the right issue is trickier, because we can't trust the year properly. Years can be off in general and journals are frequently collected into volumes, which for some reason seem to be offset by half a year. So for a journal in 1936 there are three years to search: 1935, in case the original number was published in 1936 but part of a volume starting in 1935, 1936, and 1937 (in case the converse of the first case holds). So we get the TOC of all three journals, and parse it. Unfortunately now we discover that Gallica's API excellence is only skin-deep: the TOC is only available in an XML more akin to HTML than a machine-friendly index to a document. So it has to be scraped, and then we look to see whether or not there is an article with the right title beginning on the right page: if so, the issue is a match. If this fails (perhaps because there is no TOC) we try fetching the page the article ought to be on, and doing a fuzzy search for the title. (Fortunately Gallica provides a pagination api to convert page numbers into views. Unfortunately it returns unstructured html, with the conversion as a bunch of cells in a list, so we iterate over them. We don't do this often enough for it to be worth building a mapping.) If this matches we try a fuzzy search for the author on the first page of the article, and the last if that fails. These matches are scored as the levenshtein distance (ratio) of the longest matching fuzzy string against the target, so matching 'de la' will not cause a false positive. On the other hand for authors with short names you really do need to supply them the way they sign themselves, since there's no automagic way to work out an abbreviation (at any rate, no easy way to do it which wouldn't trigger false positives without a much cleverer heuristic than Levenshtein distance). For long/obscure names this shouldn't be a problem. As an example, Marie-Dominique Chenu failed for M.-D. Chenu , but R\u00e9ginald Garrigou-Lagrange matched R. Garrigou-Lagrange O. P. without any difficulty. (Can you tell I'm a theologian?) This text matching uses the ocr text endpoint. Unfortunately, this returns the unformatted plaintext wrapped in an html document . The server crashes 1 if you requests text/plain or application/xml , but if you ask for application/json it serves up the same html page in a json object (without decomposing it at all). So out comes Beautiful Soup again, and we can get the text back out of the html. This is not quite using a regex to parse html , though actually the source is simple enough we could probably do that. Let's not, though.","title":"Article Search Algorithm"},{"location":"matching/#matching-other-resources","text":"Is rather easier, since we basically just use the journal matching algorithm above. All the pieces are in place to do this, but it has not yet been tested, as I have not yet wanted to fetch a book or journal which Gallica holds.","title":"Matching other resources"},{"location":"matching/#match-objects","text":"We could have just put this algorithm in a function somewhere. But tuning heuristics when everything's in a function gets difficult, so we use Match() objects to keep track of the various different criteria. Currently everything gets equal weighting, but this will doubtless require tuning as time goes on. We also set a minimum threshold below which to fail. Note, crashes, and gives a very unhelpful error message. \u21a9","title":"Match() objects"},{"location":"output/","text":"Output Output is by means of jinja2 templates, which have access to Result() objects containing very nearly every step of the process, so output can be quite powerful. For more ideas, see the example templates and the demosite template .","title":"Output"},{"location":"output/#output","text":"Output is by means of jinja2 templates, which have access to Result() objects containing very nearly every step of the process, so output can be quite powerful. For more ideas, see the example templates and the demosite template .","title":"Output"}]}